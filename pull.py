import argparse
import asyncio
import logging
import os
import re
import time
from collections import defaultdict
from typing import Dict, List, Tuple

import aiohttp
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# è¯·æ±‚å¤´é…ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 '
                  'Safari/537.36',
    'Referer': 'https://bangumi.tv/'
}

# æ­£åˆ™è¡¨è¾¾å¼é¢„ç¼–è¯‘
EPS_PATTERN = re.compile(r'(\d+)è¯')
FULL_DATE_PATTERN = re.compile(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥')
YEAR_MONTH_PATTERN = re.compile(r'(\d{4})å¹´(\d{1,2})æœˆ')
YEAR_PATTERN = re.compile(r'(\d{4})å¹´')
# æ–°å¢çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºåŒ¹é…æ ¼å¼åŒ–çš„æ—¥æœŸ
FORMATTED_DATE_PATTERN = re.compile(r'(\d{4})-(\d{1,2})-(\d{1,2})(?:\(.*?\))?')
FORMATTED_YEAR_MONTH_PATTERN = re.compile(r'(\d{4})-(\d{1,2})(?:\(.*?\))?')

# å¹¶å‘æ§åˆ¶é…ç½®
DEFAULT_CONCURRENT = 5
MAX_CONCURRENT = int(os.environ.get('CONCURRENT_REQUESTS', DEFAULT_CONCURRENT))


class BangumiScraper:
    def __init__(self):
        self.semaphore = None
        self.connector = None
        self.current_year = time.localtime().tm_year
        self.current_month = time.localtime().tm_mon

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT)
        self.connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, ssl=False)
        return self

    async def __aexit__(self, exc_type, exc, tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.connector.close()

    async def fetch_pages(self, session: aiohttp.ClientSession, url: str) -> int:
        """è·å–æ€»é¡µæ•°"""
        retries = 3
        while retries > 0:
            try:
                async with session.get(url, headers=HEADERS, timeout=20) as resp:
                    if resp.status != 200:
                        raise aiohttp.ClientResponseError(
                            resp.request_info, resp.history, status=resp.status)

                    soup = BeautifulSoup(await resp.text(), 'lxml')
                    pagination = soup.select_one('.page_inner')

                    if not pagination:
                        return 1

                    last_page = 1
                    for page in pagination.select('a.p'):
                        if page.text.isdigit():
                            last_page = max(last_page, int(page.text))
                    return last_page

            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                retries -= 1
                logging.info(f"è·å–é¡µæ•°å¤±è´¥: {str(e)}ï¼Œå‰©ä½™é‡è¯•æ¬¡æ•°: {retries}")
                await asyncio.sleep(2 + retries * 3)
        return 0

    async def scrape_page(self, session: aiohttp.ClientSession, base_url: str, page: int, year: int,
                          month: int = None) -> List[Dict]:
        """çˆ¬å–å•ä¸ªé¡µé¢"""
        url = f"{base_url}&page={page}"
        logging.info(f"æ­£åœ¨çˆ¬å–: {url}")

        try:
            async with self.semaphore:
                async with session.get(url, headers=HEADERS, timeout=20) as resp:
                    resp.raise_for_status()
                    soup = BeautifulSoup(await resp.text(), 'lxml')
                    return self.parse_page(soup, year, month)
        except Exception as e:
            logging.info(f"é¡µé¢çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {str(e)}")
            return []

    def parse_page(self, soup: BeautifulSoup, base_year: int, base_month: int = None) -> List[Dict]:
        """è§£æé¡µé¢å†…å®¹ï¼ˆä¿®å¤æ ‡é¢˜å’Œå°é¢é—®é¢˜ï¼‰"""
        results = []
        for item in soup.select('#browserItemList > li.item'):
            anime = defaultdict(str)

            # æ ‡é¢˜ä¿¡æ¯å¤„ç†
            title_tag = item.select_one('h3 > a.l')
            if title_tag:
                # æå–ä¸­æ–‡æ ‡é¢˜ï¼ˆä¸»æ ‡é¢˜ï¼‰
                anime['title'] = title_tag.text.strip()
                anime['url'] = f"https://bangumi.tv{title_tag['href']}"

                # æå–æ—¥æ–‡æ ‡é¢˜ï¼ˆå‰¯æ ‡é¢˜ï¼‰
                if jp_title := title_tag.find_next_sibling('small', class_='grey'):
                    anime['jp_title'] = jp_title.text.strip()

            # å°é¢å›¾ç‰‡å¤„ç†
            if img := item.select_one('a.subjectCover img.cover'):
                # ä¿®å¤å°é¢URLåè®®é—®é¢˜
                cover_url = img.get('src') or img.get('data-cfsrc', '')
                if cover_url.startswith('//'):
                    cover_url = f"https:{cover_url}"
                anime['cover'] = cover_url

            # å…ƒæ•°æ®è§£æ
            self.parse_metadata(item.select_one('p.info.tip'), anime, base_year, base_month)
            self.parse_rating(item.select_one('p.rateInfo'), anime)

            results.append(anime)
        return results

    def parse_metadata(self, elem: BeautifulSoup, anime: Dict, base_year: int, base_month: int = None):
        """è§£æå…ƒæ•°æ®"""
        if not elem:
            return

        text = elem.text.strip()

        # åˆå§‹åŒ–é»˜è®¤å€¼
        anime.update({
            'year': base_year,
            'month': base_month or 0,
            'day': 0
        })

        # è¯æ•°æå–
        if eps := EPS_PATTERN.search(text):
            anime['episodes'] = eps.group(1)

        # æ—¥æœŸè§£æ - æŒ‰ä¼˜å…ˆé¡ºåºå°è¯•ä¸åŒæ ¼å¼
        # 1. å…ˆå°è¯•åŒ¹é…å®Œæ•´çš„æ ¼å¼åŒ–æ—¥æœŸ YYYY-MM-DD
        if formatted_date := FORMATTED_DATE_PATTERN.search(text):
            anime.update({
                'year': int(formatted_date.group(1)),
                'month': int(formatted_date.group(2)),
                'day': int(formatted_date.group(3))
            })
        # 2. å°è¯•åŒ¹é…ä¸­æ–‡å®Œæ•´æ—¥æœŸ YYYYå¹´MMæœˆDDæ—¥
        elif full_date := FULL_DATE_PATTERN.search(text):
            anime.update({
                'year': int(full_date.group(1)),
                'month': int(full_date.group(2)),
                'day': int(full_date.group(3))
            })
        # 3. å°è¯•åŒ¹é…æ ¼å¼åŒ–å¹´æœˆ YYYY-MM
        elif formatted_ym := FORMATTED_YEAR_MONTH_PATTERN.search(text):
            anime.update({
                'year': int(formatted_ym.group(1)),
                'month': int(formatted_ym.group(2)),
                'day': 0
            })
        # 4. å°è¯•åŒ¹é…ä¸­æ–‡å¹´æœˆ YYYYå¹´MMæœˆ
        elif ym_date := YEAR_MONTH_PATTERN.search(text):
            anime.update({
                'year': int(ym_date.group(1)),
                'month': int(ym_date.group(2)),
                'day': 0
            })
        # 5. æœ€åå°è¯•ä»…åŒ¹é…å¹´ä»½
        elif year_only := YEAR_PATTERN.search(text):
            anime['year'] = int(year_only.group(1))
            anime['month'] = 0
            anime['day'] = 0
        # 6. å¦‚æœä»¥ä¸Šéƒ½æ²¡åŒ¹é…åˆ°ï¼Œå°è¯•ç›´æ¥åŒ¹é…æ•°å­—å¹´ä»½
        elif direct_year := re.search(r'\b(\d{4})\b', text):
            anime['year'] = int(direct_year.group(1))
            anime['month'] = 0
            anime['day'] = 0

    @staticmethod
    def parse_rating(elem: BeautifulSoup, anime: Dict):
        """è§£æè¯„åˆ†ä¿¡æ¯"""
        if not elem:
            return

        if score := elem.select_one('span.number'):
            anime['score'] = score.text.strip()

        if count := elem.select_one('span.tip_j'):
            anime['votes'] = count.text.strip('()')

    async def scrape_time_range(self, session: aiohttp.ClientSession, start_year: int, end_year: int,
                                start_month: int = None, end_month: int = None) -> List[Dict]:
        """å¤„ç†æ—¶é—´èŒƒå›´çˆ¬å–"""
        all_data = []

        for year in range(start_year, end_year + 1):
            # å½“è¾“å…¥å¹´ä»½èŒƒå›´æ—¶ï¼Œå¿½ç•¥æœˆä»½å‚æ•°
            if start_year != end_year:
                months = [None]
            else:
                months = range(start_month, end_month + 1) if start_month else [None]

            for month in months:
                if month:
                    url = f"https://bangumi.tv/anime/browser/airtime/{year}-{month:02d}?sort=date"
                else:
                    url = f"https://bangumi.tv/anime/browser/airtime/{year}?sort=date"

                if year == self.current_year and month and month > self.current_month:
                    logging.info(f"è·³è¿‡æœªæ¥æœˆä»½: {year}-{month}")
                    continue

                total_pages = await self.fetch_pages(session, url)
                if total_pages == 0:
                    continue

                tasks = [self.scrape_page(session, url, p, year, month)
                         for p in range(1, total_pages + 1)]
                results = await asyncio.gather(*tasks)
                all_data.extend(
                    [item for sublist in results for item in sublist])

        return all_data

    def generate_markdown(self, new_data: List[Dict], filename: str = "Bangumi_Anime.md"):
        """ç”Ÿæˆæˆ–æ›´æ–°MarkdownæŠ¥å‘Š"""
        # åˆå¹¶ç°æœ‰æ•°æ®
        existing_data = self.parse_existing_markdown(
            filename) if os.path.exists(filename) else []

        # è®°å½•æ–°æ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
        new_items_count = 0
        new_years_data = defaultdict(int)

        # åˆå¹¶æ•°æ®å¹¶è·Ÿè¸ªæ–°å¢æ¡ç›®
        merged_data = []
        seen = set()

        # å¤„ç†ç°æœ‰æ•°æ®
        for item in existing_data:
            identifier = (
                item.get('year'),
                item.get('title'),
                item.get('episodes'),
                item.get('url', '').split('/')[-1] if item.get('url') else ''
            )
            if identifier not in seen:
                seen.add(identifier)
                merged_data.append(item)

        # å¤„ç†æ–°æ•°æ®
        for item in new_data:
            identifier = (
                item.get('year'),
                item.get('title'),
                item.get('episodes'),
                item.get('url', '').split('/')[-1] if item.get('url') else ''
            )
            if identifier not in seen:
                seen.add(identifier)
                new_items_count += 1
                new_years_data[item.get('year')] += 1
                merged_data.append({
                    'year': item.get('year', 0),
                    'month': item.get('month', 0),
                    'day': item.get('day', 0),
                    'cover': item.get('cover', ''),
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'jp_title': item.get('jp_title', ''),
                    'episodes': item.get('episodes', 'æœªçŸ¥'),
                    'score': item.get('score', '-'),
                    'votes': item.get('votes', '0')
                })

        # æŒ‰å¹´ä»½åˆ†ç»„
        year_dict = defaultdict(list)
        for item in merged_data:
            year = item.get('year', 'æœªçŸ¥')
            year_dict[year].append(item)

        # åˆ›å»ºç›®å½•ç»“æ„
        md_content = "# Bangumiç•ªå‰§æ•°æ®æŠ¥å‘Š\n\n## ç›®å½•\n"
        sorted_years = sorted(year_dict.keys(),
                              key=lambda y: int(y) if str(y).isdigit() else 0,
                              reverse=True)

        # ç”Ÿæˆç›®å½•
        for year in sorted_years:
            md_content += f"- [{year}å¹´](#{year}å¹´)\n"
        md_content += "\n"

        # ç”Ÿæˆè¯¦ç»†å†…å®¹
        for year in sorted_years:
            md_content += f"## {year}å¹´\n\n"
            md_content += "| æ”¾é€æ—¥æœŸ | å°é¢ | ä¸­æ–‡æ ‡é¢˜ | æ—¥æ–‡æ ‡é¢˜ | è¯æ•° | è¯„åˆ† | è¯„åˆ†äººæ•° |\n"
            md_content += "| --- | --- | --- | --- | --- | --- | --- |\n"

            # æŒ‰æ—¥æœŸæ’åº
            sorted_items = sorted(
                year_dict[year],
                key=lambda x: (-x['year'], -
                x.get('month', 0), -x.get('day', 0))
            )

            # ç”Ÿæˆè¡¨æ ¼è¡Œ
            for item in sorted_items:
                # æ—¥æœŸæ ¼å¼åŒ–
                date_parts = []
                if item.get('year'):
                    date_parts.append(f"{item['year']}")
                    if item.get('month') and item['month'] > 0:  # ç¡®ä¿æœˆä»½æœ‰æ•ˆ
                        date_parts.append(f"{item['month']:02d}")
                        if item.get('day') and item['day'] > 0:  # ç¡®ä¿æ—¥æœŸæœ‰æ•ˆ
                            date_parts.append(f"{item['day']:02d}")
                date_str = "-".join(date_parts) if date_parts else "æœªçŸ¥"

                # å°é¢å¤„ç†
                cover = f"![]({item['cover']})" if item.get('cover') else ""

                # æ ‡é¢˜é“¾æ¥
                ch_title = item.get('title', 'æœªçŸ¥æ ‡é¢˜').strip()
                title_link = f"[{ch_title}]({item.get('url', '')})" if item.get(
                    'url') else ch_title

                # æ—¥æ–‡æ ‡é¢˜å¤„ç†
                jp_title = item.get('jp_title', '').strip()

                # è¯„åˆ†äººæ•°å¤„ç†
                votes = re.sub(r'\D', '', item.get('votes', '0'))  # æå–çº¯æ•°å­—
                votes = votes if votes else '0'

                md_content += f"| {date_str} | {cover} | {title_link} | {jp_title} | " \
                              f"{item.get('episodes', 'æœªçŸ¥')} | {item.get('score', '-')} | " \
                              f"{votes} |\n"
            md_content += "\n"

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logging.info("âœ… æ•°æ®åˆå¹¶å®Œæˆ:")
        logging.info(f"   - ç°æœ‰æ•°æ®: {len(existing_data)} æ¡")
        logging.info(f"   - æœ¬æ¬¡æ–°å¢: {new_items_count} æ¡")

        # æŒ‰å¹´ä»½æ˜¾ç¤ºæ–°å¢æ•°æ®ç»Ÿè®¡
        if new_items_count > 0:
            logging.info("   - æ–°å¢æ•°æ®å¹´ä»½åˆ†å¸ƒ:")
            for year, count in sorted(new_years_data.items(), reverse=True):
                logging.info(f"     * {year}å¹´: {count} æ¡")

        # å†™å…¥æ–‡ä»¶
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"ğŸ“ æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.abspath(filename)}")

    def parse_existing_markdown(self, filename: str) -> List[Dict]:
        """è§£æç°æœ‰Markdownæ–‡ä»¶"""
        existing_data = []
        current_year = None
        line_count = 0
        parsed_count = 0

        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    line_count += 1
                    line = line.strip()

                    # åŒ¹é…å¹´ä»½æ ‡é¢˜
                    if line.startswith('## '):
                        year_match = re.match(r'## (\d+)å¹´', line)
                        if year_match:
                            current_year = int(year_match.group(1))

                    # åŒ¹é…è¡¨æ ¼è¡Œ
                    elif line.startswith('|') and not line.startswith(('| ---', '| æ”¾é€æ—¥æœŸ')):
                        parts = [p.strip() for p in line.split('|')[1:-1]]
                        if len(parts) >= 6:
                            try:
                                # è§£æå°é¢URL
                                cover = ''
                                if '![]' in parts[1]:
                                    cover_match = re.search(
                                        r'!\[\]\((.*?)\)', parts[1])
                                    if cover_match:
                                        cover = cover_match.group(1)

                                # è§£ææ ‡é¢˜å’ŒURL
                                title = parts[2]
                                url = ''
                                if '[' in parts[2] and '](' in parts[2]:
                                    title_match = re.search(r'\[(.*?)\]', parts[2])
                                    url_match = re.search(r'\]\((.*?)\)', parts[2])
                                    if title_match:
                                        title = title_match.group(1)
                                    if url_match:
                                        url = url_match.group(1)

                                # åˆå§‹åŒ–æ¡ç›®ï¼Œç¡®ä¿æœ‰é»˜è®¤å€¼
                                item = {
                                    'year': current_year,
                                    'month': 0,
                                    'day': 0,
                                    'cover': cover,
                                    'title': title,
                                    'url': url,
                                    'jp_title': parts[3],
                                    'episodes': parts[4],
                                    'score': parts[5],
                                    'votes': parts[6] if len(parts) > 6 else '0'
                                }

                                # è§£ææ—¥æœŸï¼ˆåŠ å¼ºæ—¥æœŸè§£æï¼‰
                                date_str = parts[0]
                                date_parts = date_str.split('-')

                                try:
                                    if len(date_parts) >= 1:
                                        # å¤„ç†çº¯å¹´ä»½æ ¼å¼
                                        if date_parts[0].isdigit():
                                            item['year'] = int(date_parts[0])

                                    if len(date_parts) >= 2:
                                        # å¤„ç†å¹´-æœˆæ ¼å¼
                                        if date_parts[1].isdigit():
                                            item['month'] = int(date_parts[1])

                                    if len(date_parts) >= 3:
                                        # å¤„ç†å¹´-æœˆ-æ—¥æ ¼å¼
                                        if date_parts[2].isdigit():
                                            item['day'] = int(date_parts[2])
                                        # å¤„ç†å¯èƒ½å«æœ‰æ‹¬å·çš„æƒ…å†µ, å¦‚ "25(ç¾å›½)"
                                        elif '(' in date_parts[2]:
                                            day_part = date_parts[2].split('(')[0]
                                            if day_part.isdigit():
                                                item['day'] = int(day_part)
                                except ValueError:
                                    # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä¿ç•™å½“å‰å¹´ä»½
                                    item['year'] = current_year

                                existing_data.append(item)
                                parsed_count += 1
                            except Exception as e:
                                logging.error(f"âš ï¸ è§£æè¡Œå‡ºé”™: {line[:50]}... | é”™è¯¯: {str(e)}")
                                continue

            logging.info(f"âœ… è§£ææ—§æ•°æ®å®Œæˆ | æ€»è¡Œæ•°: {line_count} | è§£ææ¡ç›®: {parsed_count}")
            return existing_data
        except Exception as e:
            logging.error(f"âŒ è§£ææ–‡ä»¶å‡ºé”™: {str(e)}")
            # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œç¡®ä¿ç¨‹åºå¯ä»¥ç»§ç»­è¿è¡Œ
            return []

    @staticmethod
    def merge_data(existing: List[Dict], new: List[Dict]) -> List[Dict]:
        """åˆå¹¶å¹¶å»é‡æ•°æ®"""
        seen = set()
        merged = []

        # å¤„ç†ç°æœ‰æ•°æ®
        for item in existing:
            identifier = (
                item.get('year'),
                item.get('title'),
                item.get('episodes'),
                item.get('url', '').split('/')[-1] if item.get('url') else ''
            )
            if identifier not in seen:
                seen.add(identifier)
                merged.append(item)

        # å¤„ç†æ–°æ•°æ®
        for item in new:
            identifier = (
                item.get('year'),
                item.get('title'),
                item.get('episodes'),
                item.get('url', '').split('/')[-1] if item.get('url') else ''
            )
            if identifier not in seen:
                seen.add(identifier)
                merged.append({
                    'year': item.get('year', 0),
                    'month': item.get('month', 0),
                    'day': item.get('day', 0),
                    'cover': item.get('cover', ''),
                    'title': item.get('title', ''),
                    'url': item.get('url', ''),
                    'jp_title': item.get('jp_title', ''),
                    'episodes': item.get('episodes', 'æœªçŸ¥'),
                    'score': item.get('score', '-'),
                    'votes': item.get('votes', '0')
                })

        return merged

    async def main(self):
        async with self:
            parser = argparse.ArgumentParser(description='Bangumi Scraper')
            subparsers = parser.add_subparsers(dest='mode', required=True)

            # äº¤äº’æ¨¡å¼
            interactive_parser = subparsers.add_parser(
                'interactive', help='Interactive mode for manual runs')

            # è‡ªåŠ¨æ¨¡å¼
            auto_parser = subparsers.add_parser(
                'auto', help='Automatic mode for CI/CD')
            auto_parser.add_argument(
                '--year', type=int, required=True, help='Target year')
            auto_parser.add_argument('--month', type=int, help='Target month')
            auto_parser.add_argument(
                '--concurrent', type=int, default=3, help='Concurrent requests')

            args = parser.parse_args()

            if args.mode == 'auto':
                # è‡ªåŠ¨æ¨¡å¼é€»è¾‘
                os.environ['CONCURRENT_REQUESTS'] = str(args.concurrent)
                start_year = end_year = args.year
                start_month = end_month = args.month
                logging.info(f"ğŸƒ è‡ªåŠ¨æ¨¡å¼å¯åŠ¨ | å¹´ä»½: {args.year} | æœˆä»½: {args.month or 'å…¨å¹´'}")
            else:
                # äº¤äº’æ¨¡å¼é€»è¾‘
                year_input = input("è¯·è¾“å…¥è¦çˆ¬å–çš„å¹´ä»½ï¼ˆæ”¯æŒèŒƒå›´ï¼Œå¦‚2010-2023ï¼‰: ").strip()
                start_year, end_year = self.process_year_input(year_input)

                month_input = None
                if start_year == end_year:
                    month_input = input(
                        "è¯·è¾“å…¥æœˆä»½ï¼ˆå¯é€‰ï¼Œæ”¯æŒèŒƒå›´ï¼Œå¦‚4-7ï¼‰: ").strip() or None

                start_month, end_month = self.process_month_input(
                    month_input) if month_input else (None, None)

            # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_file = "Bangumi_Anime.md"
            # ç¡®ä¿æ–‡ä»¶è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
            if not os.path.isabs(output_file):
                output_file = os.path.abspath(output_file)

            logging.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶è·¯å¾„: {output_file}")

            async with aiohttp.ClientSession(connector=self.connector) as session:
                data = await self.scrape_time_range(session, start_year, end_year, start_month, end_month)
                # ç¡®ä¿åœ¨è‡ªåŠ¨æ¨¡å¼ä¸‹ä¹Ÿèƒ½æ­£ç¡®åˆå¹¶ç°æœ‰æ•°æ®
                self.generate_markdown(data, output_file)

    @staticmethod
    def process_year_input(input_str: str) -> Tuple[int, int]:
        """å¤„ç†å¹´ä»½è¾“å…¥"""
        if not input_str:
            raise ValueError("å¿…é¡»è¾“å…¥å¹´ä»½")

        if '-' in input_str:
            parts = input_str.split('-')
            if len(parts) != 2:
                raise ValueError("æ— æ•ˆçš„å¹´ä»½èŒƒå›´æ ¼å¼")
            start, end = map(int, parts)
            return (min(start, end), max(start, end))

        if not input_str.isdigit():
            raise ValueError("å¹´ä»½å¿…é¡»ä¸ºæ•°å­—")
        return (int(input_str), int(input_str))

    @staticmethod
    def process_month_input(input_str: str) -> Tuple[int, int]:
        """å¤„ç†æœˆä»½è¾“å…¥"""
        if not input_str:
            return (None, None)

        if '-' in input_str:
            parts = input_str.split('-')
            if len(parts) != 2:
                raise ValueError("æ— æ•ˆçš„æœˆä»½èŒƒå›´æ ¼å¼")
            start, end = map(int, parts)
            if not (1 <= start <= 12 and 1 <= end <= 12):
                raise ValueError("æœˆä»½å¿…é¡»åœ¨1-12ä¹‹é—´")
            return (min(start, end), max(start, end))

        if not input_str.isdigit() or not (1 <= int(input_str) <= 12):
            raise ValueError("æ— æ•ˆçš„æœˆä»½è¾“å…¥")
        return (int(input_str), int(input_str))


async def run():
    try:
        async with BangumiScraper() as scraper:
            await scraper.main()
    except aiohttp.ClientError as e:
        logging.error(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")
    except Exception as e:
        logging.error(f"æœªçŸ¥é”™è¯¯: {str(e)}", exc_info=True)


if __name__ == "__main__":
    asyncio.run(run())
